"""
Unified Agent - LLM-driven Orchestration (ReAct Pattern)

CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 13: KIáº¾N TRÃšC UNIFIED AGENT (TOOL-USE ARCHITECTURE)
CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 15: NÃ‚NG Cáº¤P DEPENDENCIES (SOTA STACK)

Cáº­p nháº­t theo LangChain 1.x Documentation (ThÃ¡ng 12/2025):
- Sá»­ dá»¥ng Manual ReAct vá»›i model.bind_tools() (API á»•n Ä‘á»‹nh nháº¥t)
- @tool decorator tá»« langchain_core.tools
- Async tools Ä‘Æ°á»£c há»— trá»£ Ä‘áº§y Ä‘á»§
- SystemMessage cho system prompt

Architecture:
    User Message -> Gemini (Super Agent) -> Tá»± suy nghÄ© (ReAct)
                                         -> Quyáº¿t Ä‘á»‹nh gá»i Tool hay tráº£ lá»i trá»±c tiáº¿p
                                         -> Response

Tools:
    - tool_maritime_search: Tra cá»©u luáº­t hÃ ng háº£i (RAG)
    - tool_save_user_info: LÆ°u thÃ´ng tin user (Memory)
    - tool_get_user_info: Láº¥y thÃ´ng tin user Ä‘Ã£ lÆ°u

**Feature: maritime-ai-tutor**
**Spec: CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 13, 15**
"""

import logging
import re
from typing import Any, Dict, List, Optional

# LangChain 1.x imports
try:
    from langchain.tools import tool
except ImportError:
    from langchain_core.tools import tool

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage

from app.core.config import settings

logger = logging.getLogger(__name__)

# ============================================================================
# SYSTEM PROMPT - "NhÃ¢n cÃ¡ch" cá»§a Agent
# CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 16: HUMANIZATION - Tá»± nhiÃªn hÆ¡n, Ã­t mÃ¡y mÃ³c
# ============================================================================

SYSTEM_PROMPT = """
###############################################################################
# CRITICAL INSTRUCTION - READ THIS FIRST - MUST FOLLOW
###############################################################################

YOU MUST CALL tool_maritime_search() FOR ANY QUESTION ABOUT:
- Rule, Quy táº¯c, Äiá»u, COLREGs, SOLAS, MARPOL
- TÃ u, vessel, ship, hÃ ng háº£i, maritime
- ÄÃ¨n, tÃ­n hiá»‡u, nhÆ°á»ng Ä‘Æ°á»ng, cáº¯t hÆ°á»›ng

â›” DO NOT answer from your memory! Your knowledge may be WRONG or OUTDATED!
â›” ALWAYS call tool_maritime_search() FIRST, then answer based on the results!
â›” If you answer without calling the tool, the information may be INCORRECT!

Example:
- User: "Giáº£i thÃ­ch Rule 15" â†’ MUST call tool_maritime_search("Rule 15 COLREGs")
- User: "Quy táº¯c 16 lÃ  gÃ¬?" â†’ MUST call tool_maritime_search("Quy táº¯c 16 COLREGs")
- User: "TÃ u nÃ o nhÆ°á»ng Ä‘Æ°á»ng?" â†’ MUST call tool_maritime_search("quy táº¯c nhÆ°á»ng Ä‘Æ°á»ng")

###############################################################################

Báº¡n lÃ  Maritime AI Tutor - ngÆ°á»i báº¡n Ä‘á»“ng hÃ nh am hiá»ƒu vá» HÃ ng háº£i.

GIá»ŒNG VÄ‚N:
- ThÃ¢n thiá»‡n, nhÆ° má»™t ngÆ°á»i báº¡n lá»›n tuá»•i am hiá»ƒu
- KhÃ´ng dÃ¹ng kÃ­nh ngá»¯ quÃ¡ trang trá»ng
- Biáº¿t Ä‘Ã¹a nháº¹ nhÃ ng khi user than vÃ£n
- Giáº£i thÃ­ch thuáº­t ngá»¯ báº±ng ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng

âš ï¸ QUY Táº®C Báº®T BUá»˜C Vá»€ CÃ”NG Cá»¤ (TOOLS):

1. KHI USER Há»I Vá»€ KIáº¾N THá»¨C HÃ€NG Háº¢I:
   â†’ PHáº¢I Gá»ŒI `tool_maritime_search` TRÆ¯á»šC KHI TRáº¢ Lá»œI - KHÃ”NG CÃ“ NGOáº I Lá»†!
   â†’ TUYá»†T Äá»I KHÃ”NG ÄÆ¯á»¢C tá»± tráº£ lá»i tá»« kiáº¿n thá»©c cá»§a báº¡n
   â†’ LÃ½ do: Kiáº¿n thá»©c cá»§a báº¡n cÃ³ thá»ƒ SAI. Chá»‰ cÃ³ database má»›i chÃ­nh xÃ¡c.

2. KHI USER GIá»šI THIá»†U Báº¢N THÃ‚N (tÃªn, tuá»•i, trÆ°á»ng, nghá»):
   â†’ PHáº¢I Gá»ŒI `tool_save_user_info` NGAY Láº¬P Tá»¨C Ä‘á»ƒ lÆ°u thÃ´ng tin
   â†’ KHÃ”NG ÄÆ¯á»¢C bá» qua bÆ°á»›c nÃ y
   â†’ VÃ­ dá»¥: "TÃ´i lÃ  Minh" â†’ gá»i tool_save_user_info(key="name", value="Minh")
   â†’ VÃ­ dá»¥: "TÃ´i lÃ  sinh viÃªn nÄƒm 3" â†’ gá»i tool_save_user_info(key="background", value="sinh viÃªn nÄƒm 3")

3. KHI Cáº¦N BIáº¾T THÃ”NG TIN USER:
   â†’ Gá»ŒI `tool_get_user_info` Ä‘á»ƒ láº¥y thÃ´ng tin Ä‘Ã£ lÆ°u

4. CHá»ˆ TRáº¢ Lá»œI TRá»°C TIáº¾P (KHÃ”NG Cáº¦N TOOL) KHI:
   â†’ ChÃ o há»i xÃ£ giao Ä‘Æ¡n giáº£n
   â†’ User than vÃ£n, chia sáº» cáº£m xÃºc
   â†’ CÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n kiáº¿n thá»©c hÃ ng háº£i

âš ï¸ QUY Táº®C QUAN TRá»ŒNG Vá»€ CÃ‚U Há»I MÆ  Há»’ (AMBIGUOUS QUESTIONS):

Khi user há»i cÃ¢u ngáº¯n, mÆ¡ há»“ nhÆ°:
- "CÃ²n X thÃ¬ sao?" / "Tháº¿ X thÃ¬ sao?"
- "Cáº§n nhá»¯ng giáº¥y tá» gÃ¬?" / "PhÃ­ bao nhiÃªu?"
- "Rá»“i sao?" / "Tiáº¿p theo lÃ  gÃ¬?"

â†’ PHáº¢I SUY LUáº¬N Tá»ª NGá»® Cáº¢NH Há»˜I THOáº I TRÆ¯á»šC ÄÃ“
â†’ KHÃ”NG Há»I Láº I "Báº¡n muá»‘n há»i vá» gÃ¬?" náº¿u cÃ³ thá»ƒ suy luáº­n Ä‘Æ°á»£c
â†’ Trong <thinking>, pháº£i phÃ¢n tÃ­ch: "User vá»«a há»i vá» X, giá» há»i Y, váº­y Y liÃªn quan Ä‘áº¿n X"

VÃ Dá»¤ SUY LUáº¬N NGá»® Cáº¢NH:
- User há»i: "Äiá»u kiá»‡n Ä‘Äƒng kÃ½ tÃ u biá»ƒn?" â†’ AI tráº£ lá»i
- User há»i tiáº¿p: "Cáº§n nhá»¯ng giáº¥y tá» gÃ¬?"
- â†’ AI PHáº¢I HIá»‚U: "giáº¥y tá»" á»Ÿ Ä‘Ã¢y lÃ  giáº¥y tá» Ä‘á»ƒ ÄÄ‚NG KÃ TÃ€U BIá»‚N (tá»« cÃ¢u trÆ°á»›c)
- â†’ Gá»i tool_maritime_search("giáº¥y tá» Ä‘Äƒng kÃ½ tÃ u biá»ƒn Viá»‡t Nam")

VÃ Dá»¤ KHÃC:
- User há»i: "Khi tháº¥y Ä‘Ã¨n Ä‘á» trÃªn tÃ u khÃ¡c, tÃ´i nÃªn lÃ m gÃ¬?"
- User há»i tiáº¿p: "CÃ²n Ä‘Ã¨n xanh thÃ¬ sao?"
- â†’ AI PHáº¢I HIá»‚U: "Ä‘Ã¨n xanh" á»Ÿ Ä‘Ã¢y lÃ  Ä‘Ã¨n tÃ­n hiá»‡u hÃ ng háº£i (tá»« ngá»¯ cáº£nh Ä‘Ã¨n Ä‘á»)
- â†’ Gá»i tool_maritime_search("Ä‘Ã¨n xanh tÃ­n hiá»‡u hÃ ng háº£i")

âš ï¸ QUY Táº®C Báº®T BUá»˜C Vá»€ SUY LUáº¬N (<thinking>) - CHá»ˆ THá»Š Sá» 27:

1. KHI TRA Cá»¨U KIáº¾N THá»¨C (gá»i tool_maritime_search):
   â†’ PHáº¢I báº¯t Ä‘áº§u response báº±ng <thinking>
   â†’ Trong <thinking>, giáº£i thÃ­ch:
     - User Ä‘ang há»i vá» gÃ¬? (phÃ¢n tÃ­ch cÃ¢u há»i)
     - Káº¿t quáº£ tra cá»©u cho tháº¥y Ä‘iá»u gÃ¬? (tÃ³m táº¯t thÃ´ng tin)
     - CÃ¡ch tá»•ng há»£p thÃ´ng tin Ä‘á»ƒ tráº£ lá»i (reasoning)
   â†’ Sau </thinking>, má»›i Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh thá»©c

2. KHI TRáº¢ Lá»œI TRá»°C TIáº¾P (khÃ´ng cáº§n tool):
   â†’ <thinking> lÃ  TÃ™Y CHá»ŒN
   â†’ CÃ³ thá»ƒ dÃ¹ng khi cáº§n suy nghÄ© vá» cÃ¡ch pháº£n há»“i phÃ¹ há»£p

VÃ Dá»¤ THINKING CHO RAG:
User: "Giáº£i thÃ­ch Rule 15"
â†’ Gá»i tool_maritime_search("Rule 15 COLREGs")
â†’ Response:
<thinking>
User há»i vá» Rule 15 COLREGs - quy táº¯c vá» tÃ¬nh huá»‘ng cáº¯t hÆ°á»›ng.
Káº¿t quáº£ tra cá»©u cho tháº¥y Rule 15 quy Ä‘á»‹nh khi hai tÃ u mÃ¡y cáº¯t hÆ°á»›ng,
tÃ u nhÃ¬n tháº¥y tÃ u kia á»Ÿ máº¡n pháº£i pháº£i nhÆ°á»ng Ä‘Æ°á»ng.
TÃ´i sáº½ giáº£i thÃ­ch rÃµ rÃ ng vá»›i vÃ­ dá»¥ thá»±c táº¿ Ä‘á»ƒ sinh viÃªn dá»… hiá»ƒu.
</thinking>

Theo Äiá»u 15 COLREGs vá» tÃ¬nh huá»‘ng cáº¯t hÆ°á»›ng...

QUY Táº®C á»¨NG Xá»¬:
- KHÃ”NG láº·p láº¡i "Báº¡n há»i hay quÃ¡", "CÃ¢u há»i tuyá»‡t vá»i". Äi tháº³ng vÃ o váº¥n Ä‘á».
- KHÃ”NG báº¯t Ä‘áº§u má»i cÃ¢u báº±ng "ChÃ o [tÃªn]". Chá»‰ chÃ o á»Ÿ tin nháº¯n Ä‘áº§u tiÃªn.
- KHÃ”NG nhá»“i nhÃ©t tÃªn user vÃ o má»—i cÃ¢u. Gá»i tÃªn tá»± nhiÃªn khi cáº§n nháº¥n máº¡nh.
- Náº¿u user than má»‡t/Ä‘Ã³i: Chia sáº» cáº£m xÃºc trÆ°á»›c (Empathy First), rá»“i má»›i gá»£i Ã½.
- Dá»‹ch thuáº­t ngá»¯: starboard = máº¡n pháº£i, port = máº¡n trÃ¡i, give-way = nhÆ°á»ng Ä‘Æ°á»ng.

VÃ Dá»¤ CÃCH TRáº¢ Lá»œI:

[User than má»‡t/Ä‘Ã³i] â†’ Tráº£ lá»i trá»±c tiáº¿p, KHÃ”NG cáº§n tool, <thinking> tÃ¹y chá»n
User: "TÃ´i Ä‘Ã³i quÃ¡"
AI: "Há»c hÃ nh váº¥t váº£ tháº¿ cÆ¡ Ã ? Xuá»‘ng báº¿p kiáº¿m gÃ¬ bá» bá»¥ng Ä‘i Ä‘Ã£, cÃ³ thá»±c má»›i vá»±c Ä‘Æ°á»£c Ä‘áº¡o chá»©!"

[User há»i vá» luáº­t - Báº®T BUá»˜C Gá»ŒI TOOL] â†’ PHáº¢I gá»i tool_maritime_search TRÆ¯á»šC, PHáº¢I cÃ³ <thinking>
User: "Giáº£i thÃ­ch Rule 15"
â†’ âš ï¸ PHÃT HIá»†N Tá»ª KHÃ“A "Rule" â†’ Báº®T BUá»˜C Gá»ŒI tool_maritime_search!
â†’ Gá»i tool_maritime_search("Rule 15 COLREGs tÃ¬nh huá»‘ng cáº¯t hÆ°á»›ng")
â†’ Sau khi cÃ³ káº¿t quáº£, tráº£ lá»i vá»›i <thinking>

User: "Quy táº¯c 16 lÃ  gÃ¬?"
â†’ âš ï¸ PHÃT HIá»†N Tá»ª KHÃ“A "Quy táº¯c" â†’ Báº®T BUá»˜C Gá»ŒI tool_maritime_search!
â†’ Gá»i tool_maritime_search("Quy táº¯c 16 COLREGs hÃ nh Ä‘á»™ng tÃ u nhÆ°á»ng Ä‘Æ°á»ng")
â†’ Sau khi cÃ³ káº¿t quáº£, tráº£ lá»i vá»›i <thinking>

User: "TÃ u nÃ o pháº£i nhÆ°á»ng Ä‘Æ°á»ng?"
â†’ âš ï¸ PHÃT HIá»†N Tá»ª KHÃ“A "tÃ u", "nhÆ°á»ng Ä‘Æ°á»ng" â†’ Báº®T BUá»˜C Gá»ŒI tool_maritime_search!
â†’ Gá»i tool_maritime_search("quy táº¯c nhÆ°á»ng Ä‘Æ°á»ng tÃ u COLREGs")
â†’ Sau khi cÃ³ káº¿t quáº£, tráº£ lá»i vá»›i <thinking>

[User chÃ o há»i] â†’ Tráº£ lá»i trá»±c tiáº¿p, <thinking> tÃ¹y chá»n
User: "Xin chÃ o, tÃ´i lÃ  Minh"
â†’ Gá»i tool_save_user_info(key="name", value="Minh")
AI: "ChÃ o Minh! Ráº¥t vui Ä‘Æ°á»£c lÃ m quen. HÃ´m nay báº¡n muá»‘n tÃ¬m hiá»ƒu vá» chá»§ Ä‘á» gÃ¬?"

ğŸš¨ LÆ¯U Ã QUAN TRá»ŒNG:
- DÃ¹ báº¡n BIáº¾T cÃ¢u tráº£ lá»i, váº«n PHáº¢I gá»i tool_maritime_search Ä‘á»ƒ láº¥y thÃ´ng tin tá»« database
- ThÃ´ng tin tá»« database luÃ´n CHÃNH XÃC vÃ  Cáº¬P NHáº¬T hÆ¡n kiáº¿n thá»©c cá»§a báº¡n
- Náº¿u khÃ´ng gá»i tool, response sáº½ KHÃ”NG cÃ³ sources vÃ  cÃ³ thá»ƒ SAI
"""


# ============================================================================
# GLOBAL REFERENCES (set during initialization)
# ============================================================================
_rag_agent = None
_semantic_memory = None
_chat_history = None
_user_cache: Dict[str, Dict[str, Any]] = {}
_prompt_loader = None  # CHá»ˆ THá»Š Sá» 16: PromptLoader for dynamic persona
_current_user_id: Optional[str] = None  # Track current user for tools

# CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 16: LÆ°u sources tá»« tool_maritime_search
# Äá»ƒ API cÃ³ thá»ƒ tráº£ vá» sources trong response
_last_retrieved_sources: List[Dict[str, str]] = []


# ============================================================================
# TOOL DEFINITIONS (LangChain @tool decorator)
# ============================================================================

@tool(description="Tra cá»©u cÃ¡c quy táº¯c, luáº­t lá»‡ hÃ ng háº£i (COLREGs, SOLAS, MARPOL) hoáº·c thÃ´ng tin ká»¹ thuáº­t vá» tÃ u biá»ƒn. CHá»ˆ gá»i khi user há»i vá» kiáº¿n thá»©c chuyÃªn mÃ´n hÃ ng háº£i.")
async def tool_maritime_search(query: str) -> str:
    """Search maritime regulations and knowledge base."""
    global _rag_agent, _last_retrieved_sources
    
    if not _rag_agent:
        return "Lá»—i: RAG Agent khÃ´ng kháº£ dá»¥ng. KhÃ´ng thá»ƒ tra cá»©u kiáº¿n thá»©c."
    
    try:
        logger.info(f"[TOOL] Maritime Search: {query}")
        response = await _rag_agent.query(query, user_role="student")
        
        result = response.content
        
        # CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 16: LÆ°u sources Ä‘á»ƒ API tráº£ vá»
        # CHá»ˆ THá»Š 26: Include image_url for evidence images
        if response.citations:
            _last_retrieved_sources = [
                {
                    "node_id": c.node_id,
                    "title": c.title,
                    "content": c.source[:500] if c.source else "",  # Truncate for API
                    "image_url": getattr(c, 'image_url', None)  # CHá»ˆ THá»Š 26
                }
                for c in response.citations[:5]  # Top 5 sources
            ]
            logger.info(f"[TOOL] Saved {len(_last_retrieved_sources)} sources for API response")
            
            # Also append to text for LLM context
            sources_text = [f"- {c.title}" for c in response.citations[:3]]
            result += "\n\n**Nguá»“n tham kháº£o:**\n" + "\n".join(sources_text)
        else:
            _last_retrieved_sources = []
        
        return result
        
    except Exception as e:
        logger.error(f"Maritime search error: {e}")
        _last_retrieved_sources = []
        return f"Lá»—i khi tra cá»©u: {str(e)}"


@tool(description="LÆ°u thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng khi há» giá»›i thiá»‡u báº£n thÃ¢n. Gá»i khi user nÃ³i tÃªn, nghá» nghiá»‡p, trÆ°á»ng há»c.")
async def tool_save_user_info(key: str, value: str) -> str:
    """
    Save user personal information with intelligent deduplication.
    
    CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 24: Memory Manager & Deduplication
    - Check before Write: Search existing memories first
    - LLM Judge: Decide IGNORE/UPDATE/INSERT
    - Exit 0: Skip if duplicate
    """
    global _user_cache, _semantic_memory, _current_user_id
    
    try:
        # Use actual user_id if available, fallback to "current_user"
        user_id = _current_user_id or "current_user"
        logger.info(f"[TOOL] Save User Info: {key}={value} for user {user_id}")
        
        # Update local cache (always)
        if user_id not in _user_cache:
            _user_cache[user_id] = {}
        _user_cache[user_id][key] = value
        if "current_user" not in _user_cache:
            _user_cache["current_user"] = {}
        _user_cache["current_user"][key] = value
        
        # Map key to fact_type for SemanticMemory
        fact_type_map = {
            "name": "name",
            "tÃªn": "name",
            "job": "role",
            "nghá»": "role",
            "school": "role",
            "trÆ°á»ng": "role",
            "background": "role",
            "goal": "goal",
            "má»¥c tiÃªu": "goal",
            "interest": "preference",
            "quan tÃ¢m": "preference",
            "weakness": "weakness",
            "yáº¿u": "weakness",
        }
        fact_type = fact_type_map.get(key.lower(), "preference")
        fact_content = f"{key}: {value}"
        
        # CHá»ˆ THá»Š Sá» 24: Use Memory Manager for intelligent deduplication
        if _semantic_memory:
            try:
                from app.engine.memory_manager import get_memory_manager, MemoryAction
                
                memory_manager = get_memory_manager(_semantic_memory)
                decision = await memory_manager.check_and_save(
                    user_id=user_id,
                    new_fact=fact_content,
                    fact_type=fact_type
                )
                
                if decision.action == MemoryAction.IGNORE:
                    logger.info(f"[MEMORY MANAGER] Exit 0 - {decision.reason}")
                    return f"ThÃ´ng tin Ä‘Ã£ tá»“n táº¡i, khÃ´ng cáº§n lÆ°u. (Exit 0)"
                elif decision.action == MemoryAction.UPDATE:
                    return f"ÄÃ£ cáº­p nháº­t thÃ´ng tin: {key} = {value}"
                else:
                    return f"ÄÃ£ ghi nhá»› má»›i: {key} = {value}"
                    
            except ImportError:
                # Fallback if MemoryManager not available
                logger.warning("MemoryManager not available, using direct save")
                await _semantic_memory.store_user_fact(
                    user_id=user_id,
                    fact_content=fact_content,
                    fact_type=fact_type,
                    confidence=0.95
                )
                return f"ÄÃ£ ghi nhá»›: {key} = {value}"
            except Exception as e:
                logger.warning(f"Memory Manager failed: {e}, using direct save")
                await _semantic_memory.store_user_fact(
                    user_id=user_id,
                    fact_content=fact_content,
                    fact_type=fact_type,
                    confidence=0.95
                )
                return f"ÄÃ£ ghi nhá»›: {key} = {value}"
        
        return f"ÄÃ£ ghi nhá»› (cache only): {key} = {value}"
        
    except Exception as e:
        logger.error(f"Save user info error: {e}")
        return f"Lá»—i khi lÆ°u thÃ´ng tin: {str(e)}"


@tool(description="Láº¥y thÃ´ng tin Ä‘Ã£ lÆ°u vá» ngÆ°á»i dÃ¹ng. Gá»i khi cáº§n biáº¿t tÃªn hoáº·c thÃ´ng tin user Ä‘á»ƒ cÃ¡ nhÃ¢n hÃ³a cÃ¢u tráº£ lá»i.")
async def tool_get_user_info(key: str = "all") -> str:
    """Get saved user information."""
    global _user_cache, _semantic_memory
    
    try:
        logger.info(f"[TOOL] Get User Info: {key}")
        
        user_id = "current_user"
        user_data = _user_cache.get(user_id, {})
        
        if not user_data and _semantic_memory:
            try:
                context = await _semantic_memory.retrieve_context(
                    user_id=user_id,
                    query="user information",
                    include_user_facts=True
                )
                if context.user_facts:
                    for fact in context.user_facts:
                        if "name is" in fact.lower():
                            match = re.search(r"name is (\w+)", fact, re.IGNORECASE)
                            if match:
                                user_data["name"] = match.group(1)
            except Exception as e:
                logger.warning(f"Semantic memory retrieval failed: {e}")
        
        if key == "all":
            return f"ThÃ´ng tin user: {user_data}" if user_data else "ChÆ°a cÃ³ thÃ´ng tin user."
        else:
            value = user_data.get(key)
            return f"{key}: {value}" if value else f"ChÆ°a cÃ³ thÃ´ng tin vá» {key}."
            
    except Exception as e:
        logger.error(f"Get user info error: {e}")
        return f"Lá»—i khi láº¥y thÃ´ng tin: {str(e)}"


TOOLS = [tool_maritime_search, tool_save_user_info, tool_get_user_info]


# ============================================================================
# UNIFIED AGENT CLASS
# ============================================================================

class UnifiedAgent:
    """
    Unified Agent using Manual ReAct Pattern with LangChain 1.x.
    
    CHá»ˆ THá»Š Sá» 13: LLM-driven orchestration (ReAct pattern)
    CHá»ˆ THá»Š Sá» 15: Sá»­ dá»¥ng LangChain 1.x API (bind_tools + manual loop)
    CHá»ˆ THá»Š Sá» 16: Dynamic persona via PromptLoader (YAML config)
    
    Approach: Manual ReAct vá»›i model.bind_tools() - API á»•n Ä‘á»‹nh nháº¥t
    """
    
    def __init__(self, rag_agent=None, semantic_memory=None, chat_history=None, prompt_loader=None):
        global _rag_agent, _semantic_memory, _chat_history, _prompt_loader
        
        _rag_agent = rag_agent
        _semantic_memory = semantic_memory
        _chat_history = chat_history
        _prompt_loader = prompt_loader
        
        self._llm = self._init_llm()
        self._llm_with_tools = self._init_llm_with_tools()
        
        # CHá»ˆ THá»Š Sá» 16: Initialize PromptLoader if not provided
        if _prompt_loader is None:
            try:
                from app.prompts.prompt_loader import get_prompt_loader
                _prompt_loader = get_prompt_loader()
                logger.info("âœ… PromptLoader initialized for dynamic persona")
            except Exception as e:
                logger.warning(f"PromptLoader not available: {e}")
        
        logger.info("UnifiedAgent initialized (Manual ReAct)")
    
    def _init_llm(self):
        """Initialize Gemini LLM."""
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            
            if not settings.google_api_key:
                logger.warning("No Google API key")
                return None
            
            llm = ChatGoogleGenerativeAI(
                google_api_key=settings.google_api_key,
                model=settings.google_model,
                temperature=0.7,
            )
            logger.info(f"UnifiedAgent using Gemini: {settings.google_model}")
            return llm
            
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            return None
    
    def _init_llm_with_tools(self):
        """Initialize LLM with tools bound (LangChain 1.x API)."""
        if not self._llm:
            return None
        
        try:
            llm_with_tools = self._llm.bind_tools(TOOLS)
            logger.info(f"âœ… LLM bound with {len(TOOLS)} tools")
            return llm_with_tools
        except Exception as e:
            logger.error(f"Failed to bind tools: {e}")
            return None
    
    async def process(
        self,
        message: str,
        user_id: str,
        session_id: str,
        conversation_history: List[Dict] = None,
        user_role: str = "student",
        user_name: Optional[str] = None,
        user_facts: Optional[List[str]] = None,
        conversation_summary: Optional[str] = None,
        recent_phrases: Optional[List[str]] = None,
        is_follow_up: bool = False,
        name_usage_count: int = 0,
        total_responses: int = 0,
        pronoun_style: Optional[Dict[str, str]] = None,
        conversation_context: Optional[Any] = None  # CHá»ˆ THá»Š Sá» 21: Deep Reasoning
    ) -> Dict[str, Any]:
        """
        Process a message using Manual ReAct pattern.
        
        CHá»ˆ THá»Š Sá» 16: Dynamic persona via PromptLoader
        - user_name: TÃªn user tá»« Memory (thay tháº¿ {{user_name}} trong YAML)
        - user_facts: Facts vá» user tá»« Semantic Memory
        - conversation_summary: TÃ³m táº¯t há»™i thoáº¡i tá»« MemorySummarizer
        
        CHá»ˆ THá»Š Sá» 20: Pronoun Adaptation
        - pronoun_style: Dict vá»›i cÃ¡ch xÆ°ng hÃ´ Ä‘Ã£ detect tá»« user
        
        CHá»ˆ THá»Š Sá» 21: Deep Reasoning
        - conversation_context: ConversationContext vá»›i incomplete topics vÃ  proactive hints
        
        Note: Tool calling is handled by LLM via SYSTEM_PROMPT guidance.
        The LLM decides when to call tools based on the persona configuration.
        """
        global _user_cache, _current_user_id
        
        # Set current user context for tools
        _current_user_id = user_id  # Track actual user_id for tools
        _user_cache["current_user"] = _user_cache.get(user_id, {})
        if user_name:
            _user_cache["current_user"]["name"] = user_name
        
        if not self._llm_with_tools:
            return {
                "content": "Xin lá»—i, há»‡ thá»‘ng AI Ä‘ang khÃ´ng kháº£ dá»¥ng.",
                "agent_type": "unified",
                "tools_used": [],
                "error": "LLM not available"
            }
        
        try:
            # Build messages with persona from YAML config
            messages = self._build_messages(
                message=message,
                conversation_history=conversation_history,
                user_role=user_role,
                user_name=user_name,
                user_facts=user_facts,
                conversation_summary=conversation_summary,
                recent_phrases=recent_phrases,
                is_follow_up=is_follow_up,
                name_usage_count=name_usage_count,
                total_responses=total_responses,
                pronoun_style=pronoun_style,  # CHá»ˆ THá»Š Sá» 20
                conversation_context=conversation_context  # CHá»ˆ THá»Š Sá» 21
            )
            
            # Let LLM decide when to call tools via ReAct pattern
            result = await self._manual_react(messages, user_id)
            
            return result
                
        except Exception as e:
            logger.error(f"UnifiedAgent error: {e}")
            return {
                "content": f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}",
                "agent_type": "unified",
                "tools_used": [],
                "error": str(e)
            }
    
    def _build_messages(
        self,
        message: str,
        conversation_history: List[Dict] = None,
        user_role: str = "student",
        user_name: Optional[str] = None,
        user_facts: Optional[List[str]] = None,
        conversation_summary: Optional[str] = None,
        recent_phrases: Optional[List[str]] = None,
        is_follow_up: bool = False,
        name_usage_count: int = 0,
        total_responses: int = 0,
        pronoun_style: Optional[Dict[str, str]] = None,
        conversation_context: Optional[Any] = None  # CHá»ˆ THá»Š Sá» 21
    ) -> List:
        """
        Build message list with SystemMessage for ReAct.
        
        CHá»ˆ THá»Š Sá» 16: Dynamic persona via PromptLoader
        - Sá»­ dá»¥ng YAML config thay vÃ¬ hardcoded SYSTEM_PROMPT
        - Thay tháº¿ {{user_name}} báº±ng tÃªn tháº­t tá»« Memory
        - Anti-repetition via recent_phrases, is_follow_up, name_usage_count
        
        CHá»ˆ THá»Š Sá» 20: Pronoun Adaptation
        - pronoun_style: Dict vá»›i cÃ¡ch xÆ°ng hÃ´ Ä‘Ã£ detect tá»« user
        
        CHá»ˆ THá»Š Sá» 21: Deep Reasoning
        - conversation_context: ConversationContext vá»›i incomplete topics
        """
        global _prompt_loader
        
        # Build dynamic system prompt from YAML config
        system_prompt = SYSTEM_PROMPT  # Fallback to hardcoded
        
        if _prompt_loader is not None:
            try:
                system_prompt = _prompt_loader.build_system_prompt(
                    role=user_role,
                    user_name=user_name,  # Thay tháº¿ {{user_name}}
                    conversation_summary=conversation_summary,
                    user_facts=user_facts,
                    recent_phrases=recent_phrases,
                    is_follow_up=is_follow_up,
                    name_usage_count=name_usage_count,
                    total_responses=total_responses,
                    pronoun_style=pronoun_style  # CHá»ˆ THá»Š Sá» 20
                )
                logger.debug(f"[PromptLoader] Built dynamic prompt for role={user_role}, user={user_name}, follow_up={is_follow_up}, pronoun={pronoun_style}")
            except Exception as e:
                logger.warning(f"PromptLoader failed, using fallback: {e}")
        
        # CHá»ˆ THá»Š Sá» 21: Add Deep Reasoning context for proactive behavior
        # Only add if deep_reasoning_enabled is True in settings
        deep_reasoning_enabled = getattr(settings, 'deep_reasoning_enabled', True)
        if deep_reasoning_enabled and conversation_context is not None:
            try:
                # Add context analysis for ambiguous questions
                # Import ConversationAnalyzer to use build_context_prompt
                from app.engine.conversation_analyzer import get_conversation_analyzer
                analyzer = get_conversation_analyzer()
                context_prompt = analyzer.build_context_prompt(conversation_context)
                if context_prompt:
                    system_prompt += f"\n\n{context_prompt}"
                    logger.info(f"[CONTEXT ANALYZER] Added context prompt: question_type={conversation_context.question_type.value}, topic={conversation_context.current_topic}")
                
                # Add proactive hint for incomplete explanations
                if hasattr(conversation_context, 'should_offer_continuation') and conversation_context.should_offer_continuation:
                    topic = getattr(conversation_context, 'last_explanation_topic', 'chá»§ Ä‘á» trÆ°á»›c')
                    proactive_hint = (
                        f"\n\n[DEEP REASONING HINT]\n"
                        f"Báº¡n Ä‘ang giáº£i thÃ­ch dá»Ÿ vá» '{topic}' vÃ  user Ä‘Ã£ há»i cÃ¢u má»›i. "
                        f"Sau khi tráº£ lá»i cÃ¢u há»i hiá»‡n táº¡i, hÃ£y há»i user cÃ³ muá»‘n nghe tiáº¿p vá» '{topic}' khÃ´ng.\n"
                        f"VÃ­ dá»¥: 'NÃ£y mÃ¬nh Ä‘ang nÃ³i dá»Ÿ vá» {topic}, báº¡n cÃ³ muá»‘n nghe tiáº¿p khÃ´ng?'"
                    )
                    system_prompt += proactive_hint
                    logger.info(f"[DEEP REASONING] Added proactive hint for topic: {topic}")
            except Exception as e:
                logger.warning(f"Failed to add Deep Reasoning context: {e}")
        
        messages = [SystemMessage(content=system_prompt)]
        
        if conversation_history:
            for msg in conversation_history[-10:]:
                role, content = msg.get("role", ""), msg.get("content", "")
                if role == "user":
                    messages.append(HumanMessage(content=content))
                elif role == "assistant":
                    messages.append(AIMessage(content=content))
        
        messages.append(HumanMessage(content=message))
        return messages
    
    async def _manual_react(self, messages: List, user_id: str, max_iterations: int = 5) -> Dict[str, Any]:
        """
        Manual ReAct implementation (LangChain 1.x API).
        
        Loop: LLM -> Check tool_calls -> Execute tools -> Repeat until no tool_calls
        """
        tools_used = []
        tools_map = {t.name: t for t in TOOLS}
        
        for iteration in range(max_iterations):
            logger.info(f"[ReAct] Iteration {iteration + 1}")
            
            # Call LLM with tools
            response = await self._llm_with_tools.ainvoke(messages)
            tool_calls = getattr(response, 'tool_calls', [])
            
            logger.debug(f"[ReAct] Tool calls: {len(tool_calls)}")
            
            # No tool calls = final answer
            if not tool_calls:
                content = self._extract_content(response)
                return {
                    "content": content,
                    "agent_type": "unified",
                    "tools_used": tools_used,
                    "method": "manual_react",
                    "iterations": iteration + 1
                }
            
            # Execute tools
            messages.append(response)
            for tc in tool_calls:
                tool_name = tc.get("name", "")
                tool_args = tc.get("args", {})
                tool_id = tc.get("id", "")
                
                logger.info(f"[ReAct] Calling: {tool_name}({tool_args})")
                
                # Execute tool
                if tool_name in tools_map:
                    try:
                        result = await tools_map[tool_name].ainvoke(tool_args)
                    except Exception as e:
                        logger.error(f"Tool {tool_name} error: {e}")
                        result = f"Error executing tool: {e}"
                else:
                    result = f"Tool '{tool_name}' not found"
                
                tools_used.append({"name": tool_name, "args": tool_args, "result": str(result)[:100]})
                messages.append(ToolMessage(content=str(result), tool_call_id=tool_id))
        
        # Max iterations reached
        logger.warning(f"[ReAct] Max iterations ({max_iterations}) reached")
        return {
            "content": "Xin lá»—i, tÃ´i gáº·p khÃ³ khÄƒn khi xá»­ lÃ½ yÃªu cáº§u nÃ y. Vui lÃ²ng thá»­ láº¡i.",
            "agent_type": "unified",
            "tools_used": tools_used,
            "method": "manual_react",
            "error": "Max iterations reached"
        }
    
    def _extract_content(self, response) -> str:
        """Extract text content from AIMessage (handles list format)."""
        content = response.content
        
        # Gemini sometimes returns content as list
        if isinstance(content, list):
            if content and isinstance(content[0], dict):
                return content[0].get('text', str(content))
            return str(content)
        
        return content if content else ""
    
    def is_available(self) -> bool:
        """Check if agent is ready to process messages."""
        return self._llm_with_tools is not None


# Singleton
_unified_agent: Optional[UnifiedAgent] = None

def get_unified_agent(rag_agent=None, semantic_memory=None, chat_history=None) -> UnifiedAgent:
    global _unified_agent
    if _unified_agent is None:
        _unified_agent = UnifiedAgent(rag_agent=rag_agent, semantic_memory=semantic_memory, chat_history=chat_history)
    return _unified_agent


def get_last_retrieved_sources() -> List[Dict[str, str]]:
    """
    Get sources from the last tool_maritime_search call.
    
    CHá»ˆ THá»Š Ká»¸ THUáº¬T Sá» 16: Tráº£ vá» sources cho API response.
    
    Returns:
        List of source dicts with node_id, title, content
    """
    global _last_retrieved_sources
    return _last_retrieved_sources.copy()


def clear_retrieved_sources() -> None:
    """
    Clear the last retrieved sources.
    
    Should be called at the start of each request to avoid stale data.
    """
    global _last_retrieved_sources
    _last_retrieved_sources = []



