# ğŸš¨ BÃO CÃO Lá»–I: Streaming API Endpoint

**NgÃ y:** 11/12/2025  
**Tá»«:** Team LMS Backend  
**Äáº¿n:** Team Backend AI  
**Má»©c Ä‘á»™:** ğŸ”´ Critical - Blocking Integration  
**Chá»§ Ä‘á»:** Streaming Endpoint `/api/v1/chat/stream` tráº£ vá» HTTP 500

---

## 1. TÃ“M Táº®T Váº¤N Äá»€

Streaming endpoint `/api/v1/chat/stream` tráº£ vá» **HTTP 500 Internal Server Error** khi gá»i tá»« LMS Backend.

Non-streaming endpoint `/api/v1/chat/` hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng.

---

## 2. CHI TIáº¾T KIá»‚M TRA

### 2.1. Health Check - âœ… OK

```bash
GET https://maritime-ai-chatbot.onrender.com/health

Response:
{
  "status": "ok",
  "database": "connected"
}
```

### 2.2. Non-Streaming Endpoint - âœ… OK

```bash
POST https://maritime-ai-chatbot.onrender.com/api/v1/chat/
Content-Type: application/json; charset=utf-8
X-API-Key: maritime-lms-prod-2024

{
  "user_id": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Hello",
  "role": "student",
  "session_id": "abc12345-e29b-41d4-a716-446655440001"
}

Response: HTTP 200 OK
{
  "status": "success",
  "data": {
    "answer": "<thinking>...",
    ...
  }
}
```

### 2.3. Streaming Endpoint - âŒ HTTP 500

```bash
POST https://maritime-ai-chatbot.onrender.com/api/v1/chat/stream
Content-Type: application/json; charset=utf-8
X-API-Key: maritime-lms-prod-2024
Accept: text/event-stream

{
  "user_id": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Hello",
  "role": "student",
  "session_id": "abc12345-e29b-41d4-a716-446655440001"
}

Response: HTTP 500 Internal Server Error
```

---

## 3. THÃ”NG TIN MÃ”I TRÆ¯á»œNG TEST

| Item | Value |
|------|-------|
| Thá»i gian test | 11/12/2025, ~01:30 AM (GMT+7) |
| AI Service URL | https://maritime-ai-chatbot.onrender.com |
| API Key | maritime-lms-prod-2024 |
| Test Tool | PowerShell Invoke-RestMethod |
| Request Format | JSON vá»›i UTF-8 encoding |

---

## 4. REQUEST BODY ÄÃƒ Sá»¬ Dá»¤NG

```json
{
  "user_id": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Hello",
  "role": "student",
  "session_id": "abc12345-e29b-41d4-a716-446655440001"
}
```

**LÆ°u Ã½:** Request body giá»‘ng há»‡t vá»›i non-streaming endpoint (Ä‘ang hoáº¡t Ä‘á»™ng).

---

## 5. CÃ‚U Há»I CHO TEAM AI

1. **Streaming endpoint Ä‘Ã£ Ä‘Æ°á»£c deploy chÆ°a?**
   - Theo document `PHANHOI_STREAMING_API_20251211.md`, status lÃ  "â³ Pending push"

2. **CÃ³ thá»ƒ check server logs Ä‘á»ƒ xem chi tiáº¿t lá»—i 500?**
   - Cáº§n biáº¿t root cause Ä‘á»ƒ debug

3. **Request format cÃ³ khÃ¡c gÃ¬ so vá»›i non-streaming khÃ´ng?**
   - Hiá»‡n táº¡i Ä‘ang dÃ¹ng cÃ¹ng format

4. **CÃ³ cáº§n thÃªm headers Ä‘áº·c biá»‡t nÃ o khÃ´ng?**
   - ÄÃ£ thá»­ vá»›i `Accept: text/event-stream`

---

## 6. TÃC Äá»˜NG Äáº¾N LMS

### Hiá»‡n táº¡i:
- âœ… Non-streaming chat hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
- âœ… ÄÃ£ implement fallback: **Fake Streaming (Typewriter Effect)**
- âš ï¸ User experience khÃ´ng tá»‘i Æ°u nhÆ° real streaming

### Khi fix xong:
- Chá»‰ cáº§n Ä‘á»•i flag `USE_REAL_STREAMING = true` trong LMS Frontend
- KhÃ´ng cáº§n thay Ä‘á»•i code khÃ¡c

---

## 7. LMS IMPLEMENTATION STATUS

### Backend (Ready):
```java
// AIStreamClient.java
public Flux<ServerSentEvent<String>> streamChatSSE(AIServiceRequest request) {
    return webClient.post()
        .uri("/api/v1/chat/stream")
        .contentType(MediaType.APPLICATION_JSON)
        .accept(MediaType.TEXT_EVENT_STREAM)
        .bodyValue(request)
        .retrieve()
        .bodyToFlux(ServerSentEvent.class)
        ...
}

// AIChatController.java
@PostMapping(value = "/chat/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
public Flux<ServerSentEvent<String>> chatStream(...) {
    return aiStreamClient.streamChatSSE(aiRequest);
}
```

### Frontend (Ready):
```typescript
// chat-api.client.ts
async *streamChat(message, sessionId, context) {
    const response = await fetch('/api/v1/ai/chat/stream', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Accept': 'text/event-stream',
        },
        body: JSON.stringify(request),
    });
    // SSE parsing logic...
}

// chat.service.ts
const USE_REAL_STREAMING = false; // Waiting for AI Service fix
```

---

## 8. YÃŠU Cáº¦U HÃ€NH Äá»˜NG

| Priority | Action | Owner |
|----------|--------|-------|
| ğŸ”´ P0 | Fix streaming endpoint 500 error | Team AI |
| ğŸ”´ P0 | Check server logs for root cause | Team AI |
| ğŸŸ¡ P1 | Confirm khi fix xong | Team AI |
| ğŸŸ¢ P2 | Enable real streaming trong LMS | Team LMS |

---

## 9. TIMELINE Äá»€ XUáº¤T

| Date | Milestone |
|------|-----------|
| 11/12/2025 | Team AI investigate & fix |
| 11/12/2025 | Team AI confirm fix deployed |
| 11/12/2025 | Team LMS enable real streaming |
| 12/12/2025 | Integration testing complete |

---

## 10. LIÃŠN Há»†

**Team LMS Backend:**
- ÄÃ£ implement Ä‘áº§y Ä‘á»§ streaming support
- Äang chá» AI Service streaming endpoint hoáº¡t Ä‘á»™ng
- Fallback (fake streaming) Ä‘ang active

**Khi fix xong, vui lÃ²ng reply document nÃ y hoáº·c táº¡o document má»›i.**

---

*BÃ¡o cÃ¡o táº¡o tá»± Ä‘á»™ng bá»Ÿi LMS Backend Integration System*
