"""
Test Guardian Agent - CH·ªà TH·ªä K·ª∏ THU·∫¨T S·ªê 21

Test LLM-based content moderation.

Usage:
    cd maritime-ai-service
    python scripts/test_guardian_agent.py
"""

import asyncio
import sys
sys.path.insert(0, ".")

from dotenv import load_dotenv
load_dotenv()

from app.engine.guardian_agent import (
    GuardianAgent,
    GuardianConfig,
    GuardianDecision,
    PronounValidationResult
)


async def test_skip_patterns():
    """Test that simple greetings skip LLM."""
    print("=" * 60)
    print("TEST: Skip Patterns (Simple Greetings)")
    print("=" * 60)
    
    agent = GuardianAgent()
    
    skip_messages = [
        "Ch√†o",
        "Hello",
        "Hi",
        "Xin ch√†o",
        "C·∫£m ∆°n",
        "Thanks",
        "Bye",
    ]
    
    for msg in skip_messages:
        decision = await agent.validate_message(msg)
        status = "‚úÖ SKIP" if not decision.used_llm else "‚ùå USED LLM"
        print(f"{status}: '{msg}' -> {decision.action}")
    
    return True


async def test_pronoun_requests():
    """Test custom pronoun request validation."""
    print("\n" + "=" * 60)
    print("TEST: Custom Pronoun Requests")
    print("=" * 60)
    
    agent = GuardianAgent()
    
    test_cases = [
        # (message, should_approve)
        ("G·ªçi t√¥i l√† c√¥ng ch√∫a nh√©", True),
        ("G·ªçi t√¥i l√† thuy·ªÅn tr∆∞·ªüng", True),
        ("G·ªçi t√¥i l√† ƒë.m", False),
        ("X∆∞ng h√¥ v·ªõi t√¥i l√† ho√†ng t·ª≠", True),
        ("B·∫°n l√† tr·∫´m c·ªßa t√¥i nh√©", True),
    ]
    
    for msg, should_approve in test_cases:
        print(f"\nMessage: '{msg}'")
        print(f"Expected: {'APPROVE' if should_approve else 'REJECT'}")
        
        result = await agent.validate_pronoun_request(msg)
        
        status = "‚úÖ" if result.approved == should_approve else "‚ùå"
        print(f"{status} Result: approved={result.approved}")
        if result.approved:
            print(f"   Pronouns: user_called='{result.user_called}', ai_self='{result.ai_self}'")
        else:
            print(f"   Reason: {result.rejection_reason}")
    
    return True


async def test_contextual_filtering():
    """Test contextual content filtering."""
    print("\n" + "=" * 60)
    print("TEST: Contextual Content Filtering")
    print("=" * 60)
    
    agent = GuardianAgent()
    
    test_cases = [
        # (message, expected_action, context)
        ("C∆∞·ªõp bi·ªÉn l√† g√¨ trong h√†ng h·∫£i?", "ALLOW", "maritime"),
        ("Rule v·ªÅ piracy trong COLREGs", "ALLOW", "maritime"),
        ("M√†y l√† ƒë·ªì ngu", "BLOCK", None),
        ("Tao mu·ªën bi·∫øt v·ªÅ Rule 5", "BLOCK", None),
        ("T√†u c∆∞·ªõp bi·ªÉn c√≥ quy ƒë·ªãnh g√¨?", "ALLOW", "maritime"),
    ]
    
    for msg, expected, context in test_cases:
        print(f"\nMessage: '{msg}'")
        print(f"Context: {context or 'None'}")
        print(f"Expected: {expected}")
        
        decision = await agent.validate_message(msg, context=context)
        
        status = "‚úÖ" if decision.action == expected else "‚ùå"
        print(f"{status} Result: {decision.action}")
        if decision.reason:
            print(f"   Reason: {decision.reason}")
        print(f"   Used LLM: {decision.used_llm}, Latency: {decision.latency_ms}ms")
    
    return True


async def test_caching():
    """Test decision caching."""
    print("\n" + "=" * 60)
    print("TEST: Decision Caching")
    print("=" * 60)
    
    agent = GuardianAgent()
    
    message = "Gi·∫£i th√≠ch Rule 5 v·ªÅ quan s√°t"
    
    # First call - should use LLM
    print(f"\nFirst call: '{message}'")
    decision1 = await agent.validate_message(message)
    print(f"Used LLM: {decision1.used_llm}, Cached: {decision1.cached}, Latency: {decision1.latency_ms}ms")
    
    # Second call - should use cache
    print(f"\nSecond call (same message):")
    decision2 = await agent.validate_message(message)
    print(f"Used LLM: {decision2.used_llm}, Cached: {decision2.cached}, Latency: {decision2.latency_ms}ms")
    
    if decision2.cached:
        print("‚úÖ Caching works!")
    else:
        print("‚ùå Caching not working")
    
    return decision2.cached


async def test_fallback():
    """Test fallback to rule-based filtering."""
    print("\n" + "=" * 60)
    print("TEST: Fallback Mechanism")
    print("=" * 60)
    
    # Create agent with LLM disabled
    config = GuardianConfig(enable_llm=False)
    agent = GuardianAgent(config=config)
    
    test_cases = [
        ("Ch√†o b·∫°n", "ALLOW"),
        ("M√†y l√† ƒë·ªì ngu", "BLOCK"),
        ("Rule 5 l√† g√¨?", "ALLOW"),
    ]
    
    for msg, expected in test_cases:
        print(f"\nMessage: '{msg}'")
        print(f"Expected: {expected}")
        
        decision = await agent.validate_message(msg)
        
        status = "‚úÖ" if decision.action == expected else "‚ùå"
        print(f"{status} Result: {decision.action}")
        print(f"   Used LLM: {decision.used_llm} (should be False)")
    
    return True


async def main():
    print("\nüõ°Ô∏è GUARDIAN AGENT TEST SUITE\n")
    
    test1 = await test_skip_patterns()
    test2 = await test_pronoun_requests()
    test3 = await test_contextual_filtering()
    test4 = await test_caching()
    test5 = await test_fallback()
    
    print("\n" + "=" * 60)
    print("FINAL RESULTS")
    print("=" * 60)
    print(f"Skip Patterns: {'‚úÖ' if test1 else '‚ùå'}")
    print(f"Pronoun Requests: {'‚úÖ' if test2 else '‚ùå'}")
    print(f"Contextual Filtering: {'‚úÖ' if test3 else '‚ùå'}")
    print(f"Caching: {'‚úÖ' if test4 else '‚ùå'}")
    print(f"Fallback: {'‚úÖ' if test5 else '‚ùå'}")
    
    if all([test1, test2, test3, test4, test5]):
        print("\nüéâ ALL TESTS PASSED!")
    else:
        print("\n‚ö†Ô∏è SOME TESTS NEED ATTENTION")


if __name__ == "__main__":
    asyncio.run(main())
